services:
  internvl3-78b-vllm:
    image: nvcr.io/nvidia/tritonserver:25.04-vllm-python-py3
    container_name: internvl3-78b-vllm-server
    runtime: nvidia
    environment:
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - NVIDIA_VISIBLE_DEVICES=1,2
      - HF_HOME=/root/.cache/huggingface
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    ports:
      - "30009:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: ["python3", "-m", "vllm.entrypoints.openai.api_server", "--model", "OpenGVLab/InternVL3-78B-AWQ", "--host", "0.0.0.0", "--port", "8000", "--trust-remote-code", "--disable-async-output-proc", "--max-model-len", "16384", "--gpu-memory-utilization", "0.95", "--quantization", "awq", "--dtype", "auto", "--tensor-parallel-size", "2"]
    shm_size: '64gb'
    ulimits:
      memlock: -1
      stack: 67108864
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1', '2']
              capabilities: [gpu]